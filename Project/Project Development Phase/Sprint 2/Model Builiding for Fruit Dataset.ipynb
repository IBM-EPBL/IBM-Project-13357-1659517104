{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f902f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers,models\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3b2b786",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator (rescale = 1./255, shear_range= 0.2,zoom_range= 0.2, horizontal_flip = True)\n",
    "test_datagen =ImageDataGenerator (rescale = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14d42bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5384 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "x_train = train_datagen.flow_from_directory(\n",
    "        r'D:\\IBM\\Dataset Plant Disease\\fruit-dataset\\fruit-dataset\\train',\n",
    "        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "        batch_size=32,\n",
    "        class_mode=\"sparse\",\n",
    "#         save_to_dir=\"C:\\\\Code\\\\potato-disease-classification\\\\training\\\\AugmentedImages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62d74bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5384 images belonging to 6 classes.\n",
      "Found 1686 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "x_train = train_datagen.flow_from_directory(r'D:\\IBM\\Dataset Plant Disease\\fruit-dataset\\fruit-dataset\\train',target_size = (128,128), batch_size = 32, class_mode = 'categorical')\n",
    "x_test = test_datagen.flow_from_directory(r'D:\\IBM\\Dataset Plant Disease\\fruit-dataset\\fruit-dataset\\test',target_size = (128,128), batch_size = 32, class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57e5c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8da7eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "n_classes = 6\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.InputLayer(input_shape=input_shape),\n",
    "    layers.Conv2D(32, kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(n_classes, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1331482",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6e732c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fruit\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('fruit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d576b44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'h5py==2.10.0'\"\n"
     ]
    }
   ],
   "source": [
    "pip install 'h5py==2.10.0' --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb716e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5384 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "x_train=train_datagen.flow_from_directory(r\"D:\\IBM\\Dataset Plant Disease\\fruit-dataset\\fruit-dataset\\train\",target_size=(128,128),\n",
    "                                        class_mode='categorical',batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b8d8485",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Convolution2D,MaxPooling2D,Flatten\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b5f2af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "59dd4524",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model=Sequential()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed230326",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Convolution2D(32,(3,3),input_shape=(128,128,3),activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6aaa20c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_32 (Conv2D)           (None, 126, 126, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 63, 63, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 127008)            0         \n",
      "=================================================================\n",
      "Total params: 896\n",
      "Trainable params: 896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "679b3150",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(300,activation='relu'))\n",
    "model.add(Dense(150,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9261cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(6,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ab268091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f191ada6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "225/225 [==============================] - 29s 123ms/step - loss: 1.9747 - accuracy: 0.6633\n",
      "Epoch 2/10\n",
      "225/225 [==============================] - 21s 91ms/step - loss: 0.2402 - accuracy: 0.9174\n",
      "Epoch 3/10\n",
      "225/225 [==============================] - 20s 89ms/step - loss: 0.1824 - accuracy: 0.9362\n",
      "Epoch 4/10\n",
      "225/225 [==============================] - 37s 165ms/step - loss: 0.1474 - accuracy: 0.9482\n",
      "Epoch 5/10\n",
      "225/225 [==============================] - 24s 108ms/step - loss: 0.1321 - accuracy: 0.9523\n",
      "Epoch 6/10\n",
      "225/225 [==============================] - 22s 99ms/step - loss: 0.1264 - accuracy: 0.9582\n",
      "Epoch 7/10\n",
      "225/225 [==============================] - 22s 99ms/step - loss: 0.0806 - accuracy: 0.9706\n",
      "Epoch 8/10\n",
      "225/225 [==============================] - 21s 93ms/step - loss: 0.1078 - accuracy: 0.9584\n",
      "Epoch 9/10\n",
      "225/225 [==============================] - 43s 190ms/step - loss: 0.0912 - accuracy: 0.9673\n",
      "Epoch 10/10\n",
      "225/225 [==============================] - 59s 265ms/step - loss: 0.0600 - accuracy: 0.9780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b87f947b50>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,steps_per_epoch=len(x_train),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8147bd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fruitModel\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('fruitModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "39d6c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import Callback,EarlyStopping\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from sklearn.metrics import  classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8cb4005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (224, 224)\n",
    "INPUT_SHAPE = [224, 224, 3]\n",
    "EPOCHS = 10\n",
    "BS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "37664d11",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_buliding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [70], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel was saved\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m history\n\u001b[1;32m---> 39\u001b[0m \u001b[43mmodel_buliding\u001b[49m(model1,INPUT_SHAPE)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_buliding' is not defined"
     ]
    }
   ],
   "source": [
    "def model_building(model_name, INPUT_SHAPE=INPUT_SHAPE):\n",
    "    print('Model Initialization started')\n",
    "    base_model = model_name(include_top=False, weights='imagenet', input_shape=INPUT_SHAPE)\n",
    "    \n",
    "    for layers in base_model.layers:\n",
    "        layers.trainable = False\n",
    "    print('Model Initialization finished')\n",
    "    \n",
    "    #model creation\n",
    "    print('Model creation started')\n",
    "    inp_model = base_model.output\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(inp_model)\n",
    "    x = Dense(128, activation = 'relu')(x)\n",
    "    x = Dense(15, activation = 'sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs = base_model.input, outputs = x)\n",
    "    \n",
    "    #model summary\n",
    "    print('Model summary')\n",
    "    #model.summary()\n",
    "    \n",
    "    #model compilation\n",
    "    model.compile(optimizer = 'adam', metrics=['accuracy'], loss = 'categorical_crossentropy')\n",
    "    \n",
    "    history = model.fit(train_data_gen, validation_data=val_data_gen, \n",
    "                       validation_steps=len(val_data_gen)//BS,\n",
    "                       steps_per_epoch=len(train_data_gen)//BS,\n",
    "                       batch_size=BS, \n",
    "                       epochs=EPOCHS)\n",
    "    \n",
    "    print('Model Building Finished')\n",
    "    \n",
    "    !mkdir -p saved_model\n",
    "    model.save(f'saved_model/{model_name}_1.h5')\n",
    "    print('Model was saved')\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9ad9a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "268d051d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialization started\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "`load_weights` requires h5py when loading weights from HDF5.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [72], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vgg16_hist \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_building\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVGG16\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [70], line 3\u001b[0m, in \u001b[0;36mmodel_building\u001b[1;34m(model_name, INPUT_SHAPE)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_building\u001b[39m(model_name, INPUT_SHAPE\u001b[38;5;241m=\u001b[39mINPUT_SHAPE):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel Initialization started\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     base_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude_top\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimagenet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINPUT_SHAPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layers \u001b[38;5;129;01min\u001b[39;00m base_model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m      6\u001b[0m         layers\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\keras\\applications\\vgg16.py:222\u001b[0m, in \u001b[0;36mVGG16\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[0;32m    216\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m     weights_path \u001b[38;5;241m=\u001b[39m data_utils\u001b[38;5;241m.\u001b[39mget_file(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    219\u001b[0m         WEIGHTS_PATH_NO_TOP,\n\u001b[0;32m    220\u001b[0m         cache_subdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    221\u001b[0m         file_hash\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m6d6bbae143d832006294945121d1f1fc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 222\u001b[0m   \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m   model\u001b[38;5;241m.\u001b[39mload_weights(weights)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2219\u001b[0m, in \u001b[0;36mModel.load_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[0;32m   2217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m status\n\u001b[0;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m h5py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2219\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   2220\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`load_weights` requires h5py when loading weights from HDF5.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_graph_network \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m   2222\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2223\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnable to load weights saved in HDF5 format into a subclassed \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2224\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel which has not created its variables yet. Call the Model \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2225\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst, then load the weights.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: `load_weights` requires h5py when loading weights from HDF5."
     ]
    }
   ],
   "source": [
    "vgg16_hist = model_building(VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e00f714b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "`save_model` requires h5py.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [75], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfruitModel1.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2001\u001b[0m, in \u001b[0;36mModel.save\u001b[1;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[0;32m   1959\u001b[0m \u001b[38;5;124;03m\"\"\"Saves the model to Tensorflow SavedModel or a single HDF5 file.\u001b[39;00m\n\u001b[0;32m   1960\u001b[0m \n\u001b[0;32m   1961\u001b[0m \u001b[38;5;124;03mPlease see `tf.keras.models.save_model` or the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1998\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[0;32m   1999\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2000\u001b[0m \u001b[38;5;66;03m# pylint: enable=line-too-long\u001b[39;00m\n\u001b[1;32m-> 2001\u001b[0m \u001b[43msave\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2002\u001b[0m \u001b[43m                \u001b[49m\u001b[43msignatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_traces\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py:153\u001b[0m, in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[0;32m    144\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39m_is_graph_network \u001b[38;5;129;01mand\u001b[39;00m  \u001b[38;5;66;03m# pylint:disable=protected-access\u001b[39;00m\n\u001b[0;32m    145\u001b[0m       \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, sequential\u001b[38;5;241m.\u001b[39mSequential)):\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaving the model to HDF5 format requires the model to be a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFunctional model or a Sequential model. It does not work for \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto the Tensorflow SavedModel format (by setting save_format=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mor using `save_weights`.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 153\u001b[0m   \u001b[43mhdf5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model_to_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_optimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    156\u001b[0m   saved_model_save\u001b[38;5;241m.\u001b[39msave(model, filepath, overwrite, include_optimizer,\n\u001b[0;32m    157\u001b[0m                         signatures, options, save_traces)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\hdf5_format.py:84\u001b[0m, in \u001b[0;36msave_model_to_hdf5\u001b[1;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m\"\"\"Saves a model to a HDF5 file.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03mThe saved model contains:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03m    ImportError: if h5py is not available.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m h5py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`save_model` requires h5py.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# TODO(psv) Add warning when we save models that contain non-serializable\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# entities like metrics added using `add_metric` and losses added using\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# `add_loss.`\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39m_undeduplicated_weights):\n",
      "\u001b[1;31mImportError\u001b[0m: `save_model` requires h5py."
     ]
    }
   ],
   "source": [
    "model.save('fruitModel1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "56218421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in c:\\users\\abiz1\\.conda\\envs\\tf_test\\lib\\site-packages (3.7.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\abiz1\\.conda\\envs\\tf_test\\lib\\site-packages (from h5py) (1.23.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4d2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
